{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "from urllib.request import urlopen\n",
    "import scipy.optimize\n",
    "from scipy.spatial import distance\n",
    "import random\n",
    "from collections import defaultdict\n",
    "import nltk\n",
    "import string\n",
    "from sklearn import linear_model\n",
    "from sklearn import metrics\n",
    "from nltk.stem.porter import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseData(fname):\n",
    "  for l in open(fname):\n",
    "    yield eval(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseDataFromURL(fname):\n",
    "  for l in urlopen(fname):\n",
    "    yield eval(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data...\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "### Just the first 10000 reviews\n",
    "print(\"Reading data...\")\n",
    "# http://cseweb.ucsd.edu/classes/fa19/cse258-a/data/beer_50000.json\n",
    "data = list(parseData(\"/Users/t.z.cheng/Google_Drive/Coursework/CSE258/assignment/assignment1/train_Category.json\"))[:10000]\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "### How many unique unigrams and bigrams are there?\n",
    "## Lower case and without punctuation\n",
    "uniCount = defaultdict(int)\n",
    "biCount = defaultdict(int)\n",
    "punctuation = set(string.punctuation)\n",
    "\n",
    "for d in data:\n",
    "    r = ''.join([c for c in d['text'].lower() if not c in punctuation])\n",
    "    for w in r.split():\n",
    "        uniCount[w] += 1\n",
    "    for bi in list(nltk.bigrams(r.split())):\n",
    "        biCount[bi] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Alternative way to do it \n",
    "totalWords = 0\n",
    "biCount = defaultdict(int)\n",
    "\n",
    "for d in data:\n",
    "    r = ''.join([c for c in d['text'].lower() if not c in punctuation])\n",
    "    ws = r.split()\n",
    "#    ws = ws + [' '.join(x) for x in zip(ws[:-1],ws[1:])]\n",
    "    ws = [' '.join(x) for x in zip(ws[:-1],ws[1:])]\n",
    "    for w in ws:\n",
    "        totalWords += 1\n",
    "        biCount[w] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "bicounts = [(biCount[w], w) for w in biCount]\n",
    "bicounts.sort()\n",
    "bicounts.reverse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256618 bigrams\n",
      "Top five frequently-occuring bigrams [(4441, 'this game'), (4249, 'the game'), (3359, 'of the'), (2020, 'if you'), (2017, 'in the'), (1935, 'game is'), (1907, 'is a'), (1425, 'you can'), (1323, 'and the'), (1303, 'to the')]\n"
     ]
    }
   ],
   "source": [
    "print(len(bicounts),'bigrams')\n",
    "print('Top five frequently-occuring bigrams',bicounts[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbigrams = 1000\n",
    "bigrams = [x[1] for x in bicounts[:nbigrams]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(bigrams[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Sentiment analysis\n",
    "bigramId = dict(zip(bigrams, range(len(bigrams)))) # what is the rank of this word in the top 1000 words\n",
    "bigramSet = set(bigrams)\n",
    "\n",
    "def feature(datum):\n",
    "    feat = [0]*len(bigrams) # create a one hot encoding for whether this word present or not\n",
    "    r = ''.join([c for c in datum['text'].lower() if not c in punctuation])\n",
    "    for bi in list(nltk.bigrams(r.split())):\n",
    "        if bi in bigrams:\n",
    "            feat[bigramId[bi]] += 1\n",
    "    feat.append(1) #offset\n",
    "    return feat\n",
    "\n",
    "feat_bi = [feature(d) for d in data]\n",
    "y = [math.log2(d['hours']+1) for d in data] ### Transform hours "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 5.2424790309481235\n"
     ]
    }
   ],
   "source": [
    "# Regression\n",
    "# theta,residuals,rank,s = numpy.linalg.lstsq(X, y)\n",
    "\n",
    "# Regularized regression\n",
    "clf = linear_model.Ridge(1.0, fit_intercept=False) # MSE + 1.0 l2\n",
    "clf.fit(feat_bi, y)\n",
    "theta = clf.coef_\n",
    "predictions = clf.predict(feat_bi)\n",
    "MSE = metrics.mean_squared_error(y, predictions)\n",
    "print('MSE:', MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.0, 'you were'), (0.0, 'you will'), (0.0, 'you wont'), (0.0, 'you would'), (0.0, 'your character'), (0.0, 'your own'), (0.0, 'your time'), (0.0, 'your way'), (0.0, 'youre a'), (3.5988594999820545, 'offset_feature')]\n",
      "[(0.0, '1010 would'), (0.0, '2 is'), (0.0, 'a bad'), (0.0, 'a better'), (0.0, 'a big'), (0.0, 'a bit'), (0.0, 'a blast'), (0.0, 'a bunch'), (0.0, 'a classic'), (0.0, 'a couple')]\n"
     ]
    }
   ],
   "source": [
    "weights = list(zip(theta, bigrams + ['offset_feature']))\n",
    "weights.sort()\n",
    "print(weights[-10:])\n",
    "print(weights[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1001,)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 1001)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(feat_bi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this game', 'the game', 'of the', 'if you', 'in the', 'offset_feature']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigrams[:5] +['offset_feature']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unicounts = [(uniCount[w], w) for w in uniCount]\n",
    "unicounts.sort()\n",
    "unicounts.reverse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nWords = 1000\n",
    "words = [x[1] for x in unicounts[:nWords]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Sentiment analysis\n",
    "wordId = dict(zip(words, range(len(words)))) # what is the rank of this word in the top 1000 words\n",
    "wordSet = set(words)\n",
    "\n",
    "def feature(datum):\n",
    "  feat = [0]*len(words)\n",
    "  r = ''.join([c for c in datum['text'].lower() if not c in punctuation])\n",
    "  for w in r.split():\n",
    "    if w in words:\n",
    "      feat[wordId[w]] += 1\n",
    "  feat.append(1) #offset\n",
    "  return feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_uni = [feature(w) for w in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Combine unigram and bigram features by sum\n",
    "feat_unibi = np.array(feat_uni)+np.array(feat_bi) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regression\n",
    "# theta,residuals,rank,s = numpy.linalg.lstsq(X, y)\n",
    "\n",
    "# Regularized regression\n",
    "clf.fit(feat_unibi, y)\n",
    "theta = clf.coef_\n",
    "predictions = clf.predict(feat_unibi)\n",
    "MSE = metrics.mean_squared_error(y, predictions)\n",
    "print('MSE:', MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = list(zip(theta, words + ['offset_feature']))\n",
    "weights.sort()\n",
    "print(weights[-10:])\n",
    "print(weights[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list = ['destiny', 'annoying', 'likeable', 'chapter', 'interesting']\n",
    "word_list.sort()\n",
    "review_ID = 'r75487422'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Find the index of the review_ID\n",
    "idx = []\n",
    "for d in np.arange(0,len(data)):\n",
    "    if data[d]['reviewID'] == review_ID:\n",
    "        idx = d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Calculate IDF\n",
    "docCount = defaultdict(int)\n",
    "for d in data:\n",
    "    r = ''.join([c for c in d['text'].lower() if not c in punctuation])\n",
    "    tokens = nltk.word_tokenize(r)\n",
    "    for w in word_list:\n",
    "        if w in tokens:\n",
    "            docCount[w] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "N = len(data)\n",
    "IDF = [(w,math.log10(N / docCount[w])) for w in docCount]\n",
    "print('IDF:', IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Calculate TF\n",
    "wordCount = defaultdict(int)\n",
    "r = ''.join([c for c in data[idx]['text'].lower() if not c in punctuation])\n",
    "tokens = nltk.word_tokenize(r)\n",
    "for w in tokens:\n",
    "    if w in word_list:\n",
    "        wordCount[w] += 1\n",
    "TF = [(w,wordCount[w]) for w in wordCount]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TF.sort()\n",
    "print(TF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IDF.sort()\n",
    "print(IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TFIDF = []\n",
    "for w in np.arange(0,len(word_list)):\n",
    "    TFIDF.append(TF[w][1]*IDF[w][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_iterator = zip(word_list, TFIDF)\n",
    "print('TFDTF:',dict(zip_iterator))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Calculate IDF and TF\n",
    "word_idx_map = {w:i for i,w in enumerate(words)} \n",
    "docCount = defaultdict(int)\n",
    "TF = []\n",
    "for d in data:\n",
    "    r = ''.join([c for c in d['text'].lower() if not c in punctuation])\n",
    "    tokens = nltk.word_tokenize(r)\n",
    "    new_array = [0]*len(words)\n",
    "    for w in words: ## IDF\n",
    "        if w in tokens:\n",
    "            docCount[w] += 1\n",
    "        else: docCount[w] += 0\n",
    "    for w1 in tokens: ## TF\n",
    "        if w1 in words:\n",
    "            new_array[word_idx_map[w1]] += 1 \n",
    "    TF.append(new_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = len(data)\n",
    "IDF = [math.log10(N / docCount[w]) if docCount[w] != 0 else 0 for w in docCount ]\n",
    "print('IDF:', IDF[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TFIDF = []\n",
    "for d in np.arange(0,N):\n",
    "    tmpTF = TF[d]\n",
    "    tmpTFIDF = []\n",
    "    for w in np.arange(0,len(IDF)):\n",
    "        tmpTFIDF.append(tmpTF[w] * IDF[w])\n",
    "    tmpTFIDF.append(1)\n",
    "    TFIDF.append(tmpTFIDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regression\n",
    "# Regularized regression\n",
    "clf.fit(TFIDF, y)\n",
    "theta = clf.coef_\n",
    "predictions = clf.predict(TFIDF)\n",
    "MSE = metrics.mean_squared_error(y, predictions)\n",
    "print('MSE:', MSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TFIDF = []\n",
    "for d in np.arange(0,N):\n",
    "    tmpTF = TF[d]\n",
    "    tmpTFIDF = []\n",
    "    for w in np.arange(0,len(IDF)):\n",
    "        tmpTFIDF.append(tmpTF[w] * IDF[w])\n",
    "    TFIDF.append(tmpTFIDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r,c = np.shape(TFIDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Calculate cosine similarity for each observation  \n",
    "cos_sim = []\n",
    "for nr in np.arange(0,r):\n",
    "    cos = 1 - distance.cosine(TFIDF[idx],TFIDF[nr])\n",
    "    if np.isnan(cos): \n",
    "        cos = np.nan_to_num(cos)\n",
    "    cos_sim.append(cos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Find the top similarity\n",
    "top_cos_sim = np.argsort(cos_sim)[-2] ## -1 is ID r75487422 itself\n",
    "print('Index', top_cos_sim)\n",
    "print('cosine similarity', cos_sim[top_cos_sim])\n",
    "print('ReviewID:', data[top_cos_sim]['reviewID'])\n",
    "print('Text:', data[top_cos_sim]['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Shuffle the data to create train, test, validation sets\n",
    "train = data.copy()\n",
    "test = data.copy()\n",
    "validation = data.copy()\n",
    "np.random.shuffle(train)\n",
    "np.random.shuffle(test)\n",
    "np.random.shuffle(validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Regularization terms\n",
    "l = [0.01, 0.1, 1, 10, 100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Unigrams vs. bigrams: unigram\n",
    "wordId = dict(zip(words, range(len(words)))) # what is the rank of this word in the top 1000 words\n",
    "wordSet = set(words)\n",
    "\n",
    "def feature(datum):\n",
    "  feat = [0]*len(words)\n",
    "  r = ''.join([c for c in datum['text'].lower() if not c in punctuation])\n",
    "  for w in r.split():\n",
    "    if w in words:\n",
    "      feat[wordId[w]] += 1\n",
    "  feat.append(1) #offset\n",
    "  return feat\n",
    "train_uni = [feature(w) for w in train]\n",
    "test_uni = [feature(w) for w in test]\n",
    "valid_uni = [feature(w) for w in validation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Unigrams vs. bigrams: bigram\n",
    "bigramId = dict(zip(bigrams, range(len(bigrams)))) # what is the rank of this word in the top 1000 words\n",
    "bigramSet = set(bigrams)\n",
    "\n",
    "def feature(datum):\n",
    "    feat = [0]*len(bigrams) # create a one hot encoding for whether this word present or not\n",
    "    r = ''.join([c for c in datum['text'].lower() if not c in punctuation])\n",
    "    for bi in list(nltk.bigrams(r.split())):\n",
    "        if bi in bigrams:\n",
    "            feat[bigramId[bi]] += 1\n",
    "    feat.append(1) #offset\n",
    "    return feat\n",
    "train_bi = [feature(w) for w in train]\n",
    "test_bi = [feature(w) for w in test]\n",
    "valid_bi = [feature(w) for w in validation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = [math.log2(d['hours']+1) for d in train] ### Transform hours \n",
    "test_y = [math.log2(d['hours']+1) for d in test] ### Transform hours \n",
    "valid_y = [math.log2(d['hours']+1) for d in validation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Regularized regression of unigram\n",
    "for nl in l:\n",
    "    clf = linear_model.Ridge(nl, fit_intercept=False) # MSE + 1.0 l2\n",
    "    clf.fit(train_uni, train_y)\n",
    "    theta = clf.coef_\n",
    "    predictions = clf.predict(valid_uni)\n",
    "    MSE = metrics.mean_squared_error(valid_y, predictions)\n",
    "    print('Unigram','Regularization term:', nl, 'MSE:', MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### MSE of the testset\n",
    "clf = linear_model.Ridge(0.01, fit_intercept=False) # MSE + 1.0 l2\n",
    "clf.fit(train_uni, train_y)\n",
    "theta = clf.coef_\n",
    "predictions = clf.predict(test_uni)\n",
    "MSE = metrics.mean_squared_error(test_y, predictions)\n",
    "print('testMSE:', MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Regularized regression of bigram\n",
    "for nl in l:\n",
    "    clf = linear_model.Ridge(nl, fit_intercept=False) # MSE + 1.0 l2\n",
    "    clf.fit(train_bi, train_y)\n",
    "    theta = clf.coef_\n",
    "    predictions = clf.predict(valid_bi)\n",
    "    MSE = metrics.mean_squared_error(valid_y, predictions)\n",
    "    print('Bigram','Regularization term:', nl, 'MSE:', MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### MSE of the testset\n",
    "clf = linear_model.Ridge(0.01, fit_intercept=False) # MSE + 1.0 l2\n",
    "clf.fit(train_bi, train_y)\n",
    "theta = clf.coef_\n",
    "predictions = clf.predict(test_bi)\n",
    "MSE = metrics.mean_squared_error(test_y, predictions)\n",
    "print('testMSE:', MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Removing punctuation vs. preserving it: removing it\n",
    "wordId = dict(zip(words, range(len(words)))) # what is the rank of this word in the top 1000 words\n",
    "wordSet = set(words)\n",
    "\n",
    "def feature(datum):\n",
    "  feat = [0]*len(words)\n",
    "  r = ''.join([c for c in datum['text'].lower() if not c in punctuation])\n",
    "  for w in r.split():\n",
    "    if w in words:\n",
    "      feat[wordId[w]] += 1\n",
    "  feat.append(1) #offset\n",
    "  return feat\n",
    "train_nopunc = [feature(w) for w in train]\n",
    "test_nopunc = [feature(w) for w in test]\n",
    "valid_nopunc = [feature(w) for w in validation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Removing punctuation vs. preserving it: preserving it\n",
    "wordId = dict(zip(words, range(len(words)))) # what is the rank of this word in the top 1000 words\n",
    "wordSet = set(words)\n",
    "\n",
    "def feature(datum):\n",
    "  feat = [0]*len(words)\n",
    "  r = ''.join([c for c in datum['text'].lower()])\n",
    "  for w in r.split():\n",
    "    if w in words:\n",
    "      feat[wordId[w]] += 1\n",
    "  feat.append(1) #offset\n",
    "  return feat\n",
    "train_punc = [feature(w) for w in train]\n",
    "test_punc = [feature(w) for w in test]\n",
    "valid_punc = [feature(w) for w in validation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Regularized regression of removing punctuation\n",
    "for nl in l:\n",
    "    clf = linear_model.Ridge(nl, fit_intercept=False) # MSE + 1.0 l2\n",
    "    clf.fit(train_nopunc, train_y)\n",
    "    theta = clf.coef_\n",
    "    predictions = clf.predict(valid_nopunc)\n",
    "    MSE = metrics.mean_squared_error(valid_y, predictions)\n",
    "    print('Removing punctuation,','Regularization term:', nl, 'MSE:', MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### MSE of the testset\n",
    "clf = linear_model.Ridge(0.01, fit_intercept=False) # MSE + 1.0 l2\n",
    "clf.fit(train_nopunc, train_y)\n",
    "theta = clf.coef_\n",
    "predictions = clf.predict(test_nopunc)\n",
    "MSE = metrics.mean_squared_error(test_y, predictions)\n",
    "print('testMSE:', MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Regularized regression of preserving punctuation\n",
    "for nl in l:\n",
    "    clf = linear_model.Ridge(nl, fit_intercept=False) # MSE + 1.0 l2\n",
    "    clf.fit(train_punc, train_y)\n",
    "    theta = clf.coef_\n",
    "    predictions = clf.predict(valid_punc)\n",
    "    MSE = metrics.mean_squared_error(valid_y, predictions)\n",
    "    print('Preserving punctuation,','Regularization term:', nl, 'MSE:', MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### MSE of the testset\n",
    "clf = linear_model.Ridge(0.01, fit_intercept=False) # MSE + 1.0 l2\n",
    "clf.fit(train_punc, train_y)\n",
    "theta = clf.coef_\n",
    "predictions = clf.predict(test_punc)\n",
    "MSE = metrics.mean_squared_error(test_y, predictions)\n",
    "print('testMSE:', MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### tfidf scores vs. word counts: word counts \n",
    "wordId = dict(zip(words, range(len(words)))) # what is the rank of this word in the top 1000 words\n",
    "wordSet = set(words)\n",
    "\n",
    "def feature(datum):\n",
    "  feat = [0]*len(words)\n",
    "  r = ''.join([c for c in datum['text'].lower() if not c in punctuation])\n",
    "  for w in r.split():\n",
    "    if w in words:\n",
    "      feat[wordId[w]] += 1\n",
    "  feat.append(1) #offset\n",
    "  return feat\n",
    "train_tf = [feature(w) for w in train]\n",
    "test_tf = [feature(w) for w in test]\n",
    "valid_tf = [feature(w) for w in validation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### tfidf scores vs. word counts: tfidf scores\n",
    "### Calculate IDF and TF\n",
    "\n",
    "def feature(datum):\n",
    "    word_idx_map = {w:i for i,w in enumerate(words)} \n",
    "    docCount = defaultdict(int)\n",
    "    TF = []\n",
    "    r = ''.join([c for c in datum['text'].lower() if not c in punctuation])\n",
    "    tokens = nltk.word_tokenize(r)\n",
    "    new_array = [0]*len(words)\n",
    "    for w in words: ## IDF\n",
    "        if w in tokens:\n",
    "            docCount[w] += 1\n",
    "        else: docCount[w] += 0\n",
    "    for w1 in tokens: ## TF\n",
    "        if w1 in words:\n",
    "            new_array[word_idx_map[w1]] += 1 \n",
    "    TF.append(new_array)\n",
    "    IDF = [math.log10(N / docCount[w]) if docCount[w] != 0 else 0 for w in docCount]\n",
    "    for n in np.arange(0,len(IDF)):\n",
    "        TFIDF.append(TF[0][n]*IDF[n])\n",
    "    return TFIDF\n",
    "\n",
    "train_tfidf = [feature(w) for w in train]\n",
    "test_tfidf = [feature(w) for w in test]\n",
    "valid_tfidf = [feature(w) for w in validation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Regularized regression of word count\n",
    "for nl in l:\n",
    "    clf = linear_model.Ridge(nl, fit_intercept=False) # MSE + 1.0 l2\n",
    "    clf.fit(train_tf, train_y)\n",
    "    theta = clf.coef_\n",
    "    predictions = clf.predict(valid_tf)\n",
    "    MSE = metrics.mean_squared_error(valid_y, predictions)\n",
    "    print('Word count,','Regularization term:', nl, 'MSE:', MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### MSE of the testset\n",
    "clf = linear_model.Ridge(0.01, fit_intercept=False) # MSE + 1.0 l2\n",
    "clf.fit(train_tf, train_y)\n",
    "theta = clf.coef_\n",
    "predictions = clf.predict(test_tf)\n",
    "MSE = metrics.mean_squared_error(test_y, predictions)\n",
    "print('testMSE:', MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Regularized regression of TFIDF\n",
    "for nl in l:\n",
    "    clf = linear_model.Ridge(nl, fit_intercept=False) # MSE + 1.0 l2\n",
    "    clf.fit(train_tfidf, train_y)\n",
    "    theta = clf.coef_\n",
    "    predictions = clf.predict(valid_tfidf)\n",
    "    MSE = metrics.mean_squared_error(valid_y, predictions)\n",
    "    print('TFIDF,','Regularization term:', nl, 'MSE:', MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### MSE of the testset\n",
    "clf = linear_model.Ridge(0.01, fit_intercept=False) # MSE + 1.0 l2\n",
    "clf.fit(train_tfidf, train_y)\n",
    "theta = clf.coef_\n",
    "predictions = clf.predict(test_tfidf)\n",
    "MSE = metrics.mean_squared_error(test_y, predictions)\n",
    "print('testMSE:', MSE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
